# 1. Runtime контейнера(процессы, КЦ файлов внутри контейнера, уз используемая в контейнере)
Контейнер – это изолированный процесс, который использует основное ядро ОС. Все процессы внутри контейнера являются изолированными процессами машины, на которой запущены контейнеры. Поэтому вся информация о процессах контейнеров доступна из хостовой машины.
Получить PID главного процесса контейнера можно с помощью `crictl` – командной строки для cri -совместимых сред выполнения контейнеров. 
Получение id контейнера для последующей работы с ним:
```
crictl ps
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
d50810bafd314       173d3570a5af2       24 hours ago        Running             node-exporter             13                  d67c0025379a6       node-exporter-7m988

```
Далее необходимо получить числовой идентификатор главного процесса контейнера (PID) с помощью следующей команды:
```
crictl inspect d50810bafd314 | jq .info.pid
3511
```
Данная команда выводит данные о контейнере, с помощью `jq` фильтруем PID главного процесса контейнера.  Далее с помощью PIDа главного процесса контейнера, можно получить PIDы подпроцессов:
```
ps --ppid 3511
    PID TTY          TIME CMD
```
Процессов не найдено
В итоге в Устройства-Kubernetes-Pods(node-exporter-7m988)-Containers-node-exporter
процессы не отображаются
Но должен отображаться главный процесс с pid 3511
`   3511    3412 /bin/node_exporter --path.procfs /host/proc --path.sysfs /host/sys --collector.filesystem.ignored-mount-points "^/(sys|proc|dev|host|etc)($|/)"`

В логе видно что выполняется команда:
```
kubectl exec -it node-exporter-7m988 -c node-exporter -- sh -c 'echo "Script PID:" $$ && find /proc -maxdepth 1 -regex ".*[0-9]" | xargs ls -ld | awk '\''{print "Process ID(PID):", $NF; print "Process Permissions:", $1; print "User:", $3; print "Group:", $4; print system("cat "$NF"/status | grep ""Name:"" && echo ""Command line args:"" && cat "$NF"/cmdline && echo && echo ""Executable file path:"" && ls -l "$NF"/exe")}'\''' ; echo "end command: 5051d7c1-8655-4dac-9c8e-67d2b4efc501"
Error from server (NotFound): pods "node-exporter-7m988" not found
```
Это происходит потому что не указан namespace, если его указать то команда выполняется:
```
kubectl exec -it node-exporter-7m988 -c node-exporter -n monitoring -- sh -c 'echo "Script PID:" $$ && find /proc -maxdepth 1 -regex ".*[0-9]" | xargs ls -ld | awk '\''{print "Process ID(PID):", $NF; print "Process Permissions:", $1; print "User:", $3; print "Group:", $4; print system("cat "$NF"/status | grep ""Name:"" && echo ""Command line args:"" && cat "$NF"/cmdline && echo && echo ""Executable file path:"" && ls -l "$NF"/exe")}'\''' ; echo "end command: 5051d7c1-8655-4dac-9c8e-67d2b4efc501"
```


#### Проверка системных переменных
При проверке системных переменных в разделе Отчеты контейнера node-exporter пода node-exporter-7m988 список переменных пуст

При этом если проверять через kubernetes переменные есть
```
kubectl exec node-exporter-7m988 env -n monitoring
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=node-exporter-7m988
MY_ENV_VAR1=example_value
MY_ENV_VAR2=example_value
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
HOME=/
```
Две переменные MY_ENV_VAR1 и MY_ENV_VAR2 есть в DaemonSet
```
kubectl describe ds node-exporter -n monitoring|grep -A 2 Environment:
    Environment:
      MY_ENV_VAR1:  example_value
      MY_ENV_VAR2:  example_value
```

Также непонятно считается ли выполнение команды с некорректным результатом ошибкой. То есть выполнение команды:
```
 kubectl exec -it node-exporter-7m988 -c node-exporter -- sh -c 'echo "Script PID:" $$ && find /proc -maxdepth 1 -regex ".*[0-9]" | xargs ls -ld | awk '\''{print "Process ID(PID):", $NF; print "Process Permissions:", $1; print "User:", $3; print "Group:", $4; print system("cat "$NF"/status | grep ""Name:"" && echo ""Command line args:"" && cat "$NF"/cmdline && echo && echo ""Executable file path:"" && ls -l "$NF"/exe")}'\''' ; echo "end command: 5051d7c1-8655-4dac-9c8e-67d2b4efc501"
```
выдает ошибку
*Error from server (NotFound): pods "node-exporter-7m988" not found*
Но система обозначает это как корретную загрузку отчетов.
Далее, это не отображается в статусе Пода, контейнеров, контейнера как уведомления и статус выполнения команды видно только в разделе последняя операция.

# 2. Уязвимости Image c docker.hub(все компоненты системы, в том числе gitlab nexus)

Проверяем хеши образа nginx:1.23.2
Скачиваем образ и смотрим его параметры:
```
docker inspect nginx:1.23.2
    {
        "Id": "sha256:ac8efec875ce36b619cb41f91d9db579487b9d45ed29393dc957a745b1e0024f",
        "RepoTags": [
            "nginx:1.23.2"
        ],
        "RepoDigests": [
            "nginx@sha256:ab589a3c466e347b1c0573be23356676df90cd7ce2dbf6ec332a5f0a8b5e59db"
        ],
```
меняем образ чтобы запушить в nexus м пушим его
```
docker tag nginx:1.23.2  nexus.local.org:5001/nginx:1.23.2
docker push nexus.local.org:5001/nginx:1.23.2
```
Смотрим свойства образа
```
    {
        "Id": "sha256:ac8efec875ce36b619cb41f91d9db579487b9d45ed29393dc957a745b1e0024f",
        "RepoTags": [
            "nginx:1.23.2",
            "nexus.local.org:5001/nginx:1.23.2"
        ],
        "RepoDigests": [
            "nginx@sha256:ab589a3c466e347b1c0573be23356676df90cd7ce2dbf6ec332a5f0a8b5e59db",
            "nexus.local.org:5001/nginx@sha256:d5e4095bb4bcd2c40d6aba552f9ea66aacb1d0a5137a521dc6b0503b40b08921"
        ],
```
В RepoDigests добавиался новый хеш
Теперь то же самое делам для harbor
```
docker tag nginx:1.23.2  srv-k8s-tlkm5:443/da/nginx:1.23.2
docker push srv-k8s-tlkm5:443/da/nginx:1.23.2
```
И проверяем свойства
```
docker inspect nginx:1.23.2
 {
        "Id": "sha256:ac8efec875ce36b619cb41f91d9db579487b9d45ed29393dc957a745b1e0024f",
        "RepoTags": [
            "nginx:1.23.2",
            "nexus.local.org:5001/nginx:1.23.2",
            "srv-k8s-tlkm5:443/da/nginx:1.23.2"
        ],
        "RepoDigests": [
            "nginx@sha256:ab589a3c466e347b1c0573be23356676df90cd7ce2dbf6ec332a5f0a8b5e59db",
            "nexus.local.org:5001/nginx@sha256:d5e4095bb4bcd2c40d6aba552f9ea66aacb1d0a5137a521dc6b0503b40b08921",
            "srv-k8s-tlkm5:443/da/nginx@sha256:d5e4095bb4bcd2c40d6aba552f9ea66aacb1d0a5137a521dc6b0503b40b08921"
        ],
```
Добавился еще один RepoDigests
Проверяем в efros
Устройства - Nexus Repository-контрольные суммы образов-обновить
```
nginx:1.23.2	sha256:d5e4095bb4bcd2c40d6aba552f9ea66aacb1d0a5137a521dc6b0503b40b08921	sha256:ac8efec875ce36b619cb41f91d9db579487b9d45ed29393dc957a745b1e0024f
```
Видим что Docker-Contend-Digest равняется тому который добавился при пуше. Но Digest linux/amd64 является тем же что и у оригинального образа, параметр Id.
При этом Docker-Contend-Digest равняется manifest digest с dockerhub
А Digest linux/amd64 равняется index digest с dockerhub, что может запутать, так как по сути это id образа и он раверн для нескольких архитектур. А вот указанный Docker-Contend-Digest уникальный для разных архитектур, в частности этот для amd64.


Теперь проверяем на предмет уязвимостей
Заходим в Kubernetes 'Уязвимости Nexus Repository на основе контрольных сумм и нажимаем обновить
По контрольной сумме должны загрузиться уязвимости для образа.
В данном случае они берутся с dockerhub поиском по Docker-Contend-Digest и сравнивая его с manifest digest


Проводим такие же операции с nginx:1.12
видим в приложении:
```
nginx:1.12	sha256:09e210fe1e7f54647344d278a8d0dee8a4f59f275b72280e8b5a7c18c560057f	sha256:4037a5562b030fd80ec889bb885405587a52cfef898ffb7402649005dfda75ff
```
В dockerhub есть только manifest digest, он равен 4037a5562b030fd80ec889bb885405587a52cfef898ffb7402649005dfda75ff, также видим что данный образ не анализировался вовсе. Поэтому в разделе Уязвимости Nexus Repository на основе контрольных сумм уязвимости не найдены.
Хотя уязвимости в образе разумеется есть:
```
./trivy image nginx:1.12
.......
nginx:1.12 (debian 9.4)
Total: 292 (UNKNOWN: 9, LOW: 69, MEDIUM: 67, HIGH: 108, CRITICAL: 39)
```

# 3. Уязвимости и проверки соответствия для хостовой ОС. 

Был добавлен обьект защиты - worker нода на операционной системе Debian.
***Проверка политик безопасности***
В рамках проверки безопасности используются следующие политики безопасности:
`Политика безопасности CIS для Debian`
`Политика безопасности ФСТЭК для Linux`
`Политика безопасности прав доступа Linux`

Проверяем несколько политик безопасности прав доступа Linux
Проверяем первую небезопасную настройку прав доступа:
`Убедитесь, что доступ по протоколу SSH суперпользователем отключен`

Подключаемся к виртуальной машине по ssh и попробуем поправить конфигурацию сервиса sshd
Правим файл /etc/ssh/sshd_config
меняем параметр PermitRootLogin с уes на no далее сохраняем конфиг и перезапускаем сервис
```
sudo service ssh reload
```

Далее нажимаем загрузить все отчеты
и проверяем что количество пройденных проверок политик безопасности прав доступа увеличилось и указанная ранее небезопасная настройка ssh исчезла

***Проверка уязвимостей:***
Открываем обьекты защиты, выбираем необходимый обьект и смотрим количество уязвимостей в разделе контроль устройст
В данном случае уязвимостей нет:
`Уязвимости 0000`

Но если просканировать утилитой 
debsecan  --format report >> /home/telekom/debscan_report
уязвимости находятся.
Проверим две их них:
nano - https://security-tracker.debian.org/tracker/CVE-2024-5742
Здесь уязвимость пофикшена, но в следующей версии trixie, но это нестабильный репозитории и не вышедший релиз.

wget - https://security-tracker.debian.org/tracker/CVE-2021-31879
Здесь уязвимость не пофикшена.

Есл просматривать лог загрузки отчета то видим ошибку:
```
Ошибка: 
38	Run process '"/var/opt/efrosci/Modules/Linux(75.2)_f3cd8a14-87f8-476a-bbc8-784eb2241f99/oscap" oval eval --skip-valid --directives /tmp/c628-cf8c-e986-ecb7 --remote "telekom:Telekom7@10.72.10.172:22"  --results /tmp/e36d-eb57-23c1-67f2 /tmp/6c86-5005-d450-683b' error: Module args validation error
39	to_validate:   oval eval --skip-valid --directivestmpc628-cf8c-e986-ecb7 --remotetelekomTelekom710721017222  --results /tmp/e36d-eb57-23c1-67f2tmp6c86-5005-d450-683b
40	validate_from: oval eval --skip-valid --directives /tmp/c628-cf8c-e986-ecb7 --remote "telekom:Telekom7@10.72.10.172:22"  --results /tmp/e36d-eb57-23c1-67f2 /tmp/6c86-5005-d450-683b
```
Видим пароль в открытом виде
Не совсем понятно с какого узла выполняется команда, стоит писать `Сервер Efros` или название устройства.


